# -*- coding: utf-8 -*-
"""OCDS - Lab 11.1 - ANN - Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jfll_57SiWiIfgnlTsuwnP6p-XjbuKJy

# ANN - Classifier
"""

import keras # library for neural network. Need tensorflow to installed before performing this installation.
import pandas as pd 
import seaborn as sns 
import matplotlib.pyplot as plt 
import numpy as np 
from sklearn.preprocessing import normalize 
from sklearn.model_selection import train_test_split

# Reading data 
import pandas as pd

from google.colab import files
file = files.upload()
import io
data = pd.read_csv(io.BytesIO(file['Iris.csv']))

'''data = pd.read_csv("D:/APU/CT108-3-3 - OCDS/Lab Sessions/Lab11 - Neural Netwrok/Iris.csv")
data.head()'''

data.head()

"""**Visualisation of the dataset**

The coding below shows the visualisation of the dataset in order to understand the data more. It can be seen that every species of the Iris can be segregated into different regions to be predicted.
"""

sns.lmplot(x = 'SepalLengthCm', y = 'SepalWidthCm', data=data, fit_reg=False, hue="Species", scatter_kws={"marker": "D", "s": 50})
plt.title('SepalLength vs SepalWidth')

sns.lmplot(x = 'PetalLengthCm', y = 'PetalWidthCm', data=data, fit_reg=False, hue="Species", scatter_kws={"marker": "D", "s": 50})
plt.title('PetalLength vs PetalWidth')

sns.lmplot(x = 'SepalLengthCm', y = 'PetalLengthCm', data=data, fit_reg=False, hue="Species", scatter_kws={"marker": "D", "s": 50})
plt.title('SepalLength vs PetalLength')

sns.lmplot(x = 'SepalWidthCm', y = 'PetalWidthCm', data=data, fit_reg=False, hue="Species", scatter_kws={"marker": "D", "s": 50})
plt.title('SepalWidth vs PetalWidth')

plt.show()

"""**Coding TV**"""

print(data["Species"].unique())

print(data["Species"].value_counts())

data['Species'] = pd.factorize(data['Species'])[0]
data.head()

data = data.iloc[np.random.permutation(len(data))]
data.head()

"""**Converting data to numpy array in order for processing**"""

X = data.iloc[:,1:5].values
y = data.iloc[:,5].values

y

"""### Normalization

It can be seen from above that the feature of the first dataset has 6cm in Sepal Length, 3.4cm in Sepal Width, 4.5cm in Petal Length and 1.6cm in Petal Width. However, the range of the dataset may be different. Therefore, in order to maintain a good accuracy, the feature of each dataset must be normalized to a range of 0-1 for processing.

Still, the model can be fitted using the not normalised (scaled) data, thus may be useful for large dataset.
"""

X_normalized = normalize(X, axis=0)
print("Examples of X_normalised\n", X_normalized[:3])

# Creating train,test and validation data (using the not normalised IV
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)

print("Length of train set: ", X_train.shape, "y:", y_train.shape)
print("Length of test set: ", X_test.shape, "y:", y_test.shape)

"""### Neural network module"""

from keras.models import Sequential 
from keras.layers import Dense,Activation,Dropout 
from keras.layers.normalization import BatchNormalization 
from keras.utils import np_utils

# Change the label to one hot vector
'''
[0]--->[1 0 0]
[1]--->[0 1 0]
[2]--->[0 0 1]
'''
y_train = np_utils.to_categorical(y_train, num_classes=3)
y_test = np_utils.to_categorical(y_test, num_classes=3)
print("Shape of y_train", y_train.shape)
print("Shape of y_test", y_test.shape)

def fit_model(optimizer):
  model=Sequential()
  model.add(Dense(200, input_dim=4, activation='relu'))
  model.add(Dropout(0.2))
  model.add(Dense(100, activation='relu'))
  model.add(Dropout(0.2))
  model.add(Dense(50, activation='relu'))
  model.add(Dropout(0.2))
  model.add(Dense(3, activation='softmax'))
  
  model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
  
  history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=20, epochs=100, verbose=0)
    
  print('Accuracy with', optimizer,'optimizer is :', model.evaluate(X_test, y_test)[1]*100)
    
  plt.plot(history.history['accuracy'], label='train')
  plt.plot(history.history['val_accuracy'], label='test')
  plt.legend(loc = 'best')
  plt.title('opt=' + optimizer)

optimizers = ['sgd', 'rmsprop', 'adagrad', 'adam']
plt.figure(figsize=(10,12))
for i in range(len(optimizers)):
    # determine the plot number
    plot_no = 220 + (i+1)
    plt.subplot(plot_no)
    # fit model and plot learning curves for an optimizer
    fit_model(optimizers[i])
# show learning curves
plt.show()