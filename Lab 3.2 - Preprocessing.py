# -*- coding: utf-8 -*-
"""OCDS - Lab 3.2 - Preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WISRonnzjhHLk1DK6AOMoek6rgr5Yoxn

# Preprocessing
### Data Understandng
"""

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('/content/sample_data/Data.csv')

dataset

dataset.dtypes

dataset.shape

dataset.isnull().sum()

100 * dataset.isnull().sum() / len(dataset)

# Function to calculate missing values by column
def missing_values_table(df):
    # Total missing values
    mis_val = df.isnull().sum()
        
    # Percentage of missing values
    mis_val_percent = 100 * df.isnull().sum() / len(df)
        
    # Make a table with the results
    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)
        
    # Rename the columns
    mis_val_table_ren_columns = mis_val_table.rename(columns = {0 : 'Missing Counts', 1 : 'Percentage'})
        
    # Sort the table by percentage of missing descending
    mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(
        'Percentage', ascending=False).round(1)
            
    # Return the dataframe with missing information
    return mis_val_table_ren_columns

missing_values_table(dataset)

dataset.describe().round(2)

dataset['Country'].describe()

# Drop the rows with the missing values
dataset.dropna()

# Drop the variables with the missing values
d = dataset.dropna(axis=1)
d

"""### Imputing/Filling the missing values using fillna() method"""

dataset["Country"].fillna("France", inplace = True) 
dataset["Age"].fillna("39", inplace = True) 
dataset["Salary"].fillna("63778", inplace = True) 
dataset

dataset.isnull().sum()

"""### Imputing missing values using fillna() - using the imputation strategy"""

# Importing the dataset
import pandas as pd
dataset_1 = pd.read_csv('/content/sample_data/Data.csv')
dataset_1

dataset_1.isnull().sum()

dataset_1['Country'].value_counts()

dataset_1.fillna(dataset_1.mean(), inplace=True)
dataset_1 = dataset_1.fillna(dataset_1['Country'].value_counts().index[0])
dataset_1 = dataset_1.round()
dataset_1

"""### Imputing missing values using Imputer from Sklearn"""

# Importing the dataset
import pandas as pd
dataset_2 = pd.read_csv('D:/APU/CT108-3-3 - OCDS/Lab Sessions/Lab3 - Data Preprocessing/Data.csv')
dataset_2

"""#### Simple Imputer

https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer
"""

# Taking care of missing data
import numpy as np
from sklearn.impute import SimpleImputer
imputer_1 = SimpleImputer(missing_values=np.nan, strategy='mean')
dataset_2[['Age', 'Salary']] = imputer_1.fit_transform(dataset_2[['Age', 'Salary']])

imputer_2 = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
dataset_2[['Country']] = imputer_2.fit_transform(dataset_2[['Country']])
dataset_2 = dataset_2.round()
dataset_2

"""### Assign IV and TV"""

X = dataset[['Country', 'Age', 'Salary']]
Y = dataset['Purchased']

X

Y

"""### Label Encoding --------------------------"""

# Importing the dataset
import pandas as pd
dataset_2 = pd.read_csv('D:/APU/CT108-3-3 - OCDS/Lab Sessions/Lab3 - Data Preprocessing/Data.csv')
dataset_2

# Imputing missing values
dataset_2.fillna(dataset_2.mean(), inplace=True)
dataset_2['Country'] = dataset_2.fillna(dataset_2['Country'].value_counts().index[0])
dataset_2 = dataset_2.round()
dataset_2

# Encoding categorical data. NOTE: The missing values must be imputed if any found in the dataset
# Encoding the Independent Variable
import warnings
warnings.filterwarnings("ignore")

from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()
dataset_2['Country'] = labelencoder.fit_transform(dataset_2['Country'])
dataset_2['Purchased'] = labelencoder.fit_transform(dataset_2['Purchased'])
dataset_2

"""**The label encoding can also be done after assignign the IV & TV**

Encoding the Input and Target Variable

X_2 = dataset_2[['Country', 'Age', 'Salary']]

Y_2 = dataset_2['Purchased']

from sklearn.preprocessing import LabelEncoder

labelencoder_X = LabelEncoder()

X_2['Country'] = labelencoder_X.fit_transform(X_2['Country'])

X_2

labelencoder_Y = LabelEncoder()

Y_2 = labelencoder_Y.fit_transform(Y_2)

Y_2

**Challenges with Label Encoding**

In the above scenario, the Country names do not have an order or rank. But, when label encoding is performed, the country names are ranked based on the alphabets. Due to this, there is a very high probability that the model captures the relationship between countries such as India < Japan < the US.

This is something that we do not want! So how can we overcome this obstacle? Here comes the concept of **One-Hot Encoding**.

### Factorization using Pandas -----------------------
"""

# Importing the dataset
import pandas as pd
dataset_F = pd.read_csv('D:/APU/CT108-3-3 - OCDS/Lab Sessions/Lab3 - Data Preprocessing/Data.csv')
dataset_F

# Imputing missing values
dataset_F.fillna(dataset_F.mean(), inplace=True)
dataset_F['Country'] = dataset_F.fillna(dataset_F['Country'].value_counts().index[0])
dataset_F = dataset_F.round()
dataset_F

dataset_F['Country'] = pd.factorize(dataset_F['Country'])[0]
dataset_F['Purchased'] = pd.factorize(dataset_F['Purchased'])[0]
dataset_F

"""**Challenges with Label Encoding**

In the above scenario, the Country names do not have an order or rank. But, when label encoding is performed, the country names are ranked based on the alphabets. Due to this, there is a very high probability that the model captures the relationship between countries such as India < Japan < the US.

This is something that we do not want! So how can we overcome this obstacle? Here comes the concept of **One-Hot Encoding**.

### One Hot Encoding ----------------------

One-Hot Encoding is another popular technique for treating categorical variables. It simply creates additional features based on the number of unique values in the categorical feature. Every unique value in the category will be added as a feature.

**Using get_dummies() from Pandas**

The Countries column contain categorical values. We can convert the values in the Countries column into one-hot encoded vectors using the get_dummies() function:
"""

# Importing the dataset
import pandas as pd
dataset_d = pd.read_csv('D:/APU/CT108-3-3 - OCDS/Lab Sessions/Lab3 - Data Preprocessing/Data.csv')
dataset_d

# Imputing missing values
dataset_d.fillna(dataset_d.mean(), inplace=True)
dataset_d['Country'] = dataset_d.fillna(dataset_d['Country'].value_counts().index[0])
dataset_d = dataset_d.round()
dataset_d

pd.get_dummies(data = dataset_d, columns = ['Country', 'Purchased'])

"""**One Hot Encoding using Sklearn**"""

# Importing the dataset
import pandas as pd
dataset_one = pd.read_csv('D:/APU/CT108-3-3 - OCDS/Lab Sessions/Lab3 - Data Preprocessing/Data.csv')
dataset_one

# Imputing missing values
dataset_one.fillna(dataset_one.mean(), inplace=True)
dataset_one['Country'] = dataset_one.fillna(dataset_one['Country'].value_counts().index[0])
dataset_one = dataset_one.round()
dataset_one

from sklearn.preprocessing import OneHotEncoder

onehotencoder = OneHotEncoder()

onehot = onehotencoder.fit_transform(dataset_one.Country.values.reshape(-1,1)).toarray()

dfOneHot = pd.DataFrame(onehot, columns = ["Country_"+str(int(i)) for i in range(onehot.shape[1])]) 

dataset_one = pd.concat([dataset_one, dfOneHot], axis=1)

dataset_one = dataset_one.drop(['Country'], axis=1) 

dataset_one

"""### Feature Scaling

In many machine learning algorithms, to bring all features in the same standing, we need to do scaling so that one significant number doesnâ€™t impact the model just because of their large magnitude.

Feature scaling in machine learning is one of the most critical steps during the pre-processing of data before creating a machine learning model.

The most common techniques of feature scaling are **Normalization** and **Standardization**.

Normalization is used when we want to bound our values between two numbers, typically, between [0,1] or [-1,1]. While Standardization transforms the data to have zero mean and a variance of 1, they make our data unitless.

Reference:
https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35

**Normalization** is good to use when you know that the distribution of your data does not follow a Gaussian distribution.
"""

X_tr

from sklearn.preprocessing import MinMaxScaler
norm = MinMaxScaler().fit(X_tr)
X_train_norm = norm.transform(X_tr)
X_test_norm = norm.transform(X_te)

X_train_norm

X_test_norm

"""**Standardization**, on the other hand, can be helpful in cases where the data follows a Gaussian distribution."""

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)

X_train

X_test

"""### Data Split"""

X = dataset_one[['Age', 'Salary', 'Country_0', 'Country_1', 'Country_2']] # Input Variables
y = dataset_one['Purchased'] # Target Variable

X

y

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state=123)

X_tr, X_te, Y_tr, Y_te = train_test_split(X, y, test_size=0.30, random_state=123)

X_train

Y_train

"""### Class Balancing

Oversampling using SMOTE
"""

data_1 = pd.read_csv('/content/telecom_churn.csv')
import seaborn as sns
sns.countplot(x = data_1['Churn'])

data_1 = data_1.drop(['State', 'International plan', 'Voice mail plan'], axis = 1)

x = data_1.drop('Churn', axis = 1)
y = data_1['Churn']

from imblearn.over_sampling import SMOTE
x_b, y_b = SMOTE().fit_resample(x, y)

sns.countplot(x = y_b)

"""Undersampling using Near Miss"""

from imblearn.under_sampling import NearMiss
nm = NearMiss()
x_nm, y_nm = nm.fit_resample(x, y)

sns.countplot(x = y_nm)

"""### Homework

Select you preferred datset. Any other dataset with around 10-15 variables.
 1. Perform EDA
 2. Perform Preprocessing
"""