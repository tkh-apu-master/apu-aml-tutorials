# -*- coding: utf-8 -*-
"""OCDS - Lab 06.2 - Logistic Regression - Multinomial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BcdOV8C09-R05Q4d2rF3QKg4OpD7wtOB

# Logistic Regression - Multinomial
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from numpy import arange
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report,f1_score, roc_curve, roc_auc_score, auc

import warnings
warnings.filterwarnings("ignore")

from google.colab import files
file = files.upload()

# wine_data = pd.read_csv('D:/APU/CT108-3-3 - OCDS/Lab Sessions/Lab6 - Logistic Regression/wine.csv') 
# wine_data.head()

import io
wine_data = pd.read_csv(io.BytesIO(file['wine.csv']))
wine_data.head()

wine_data.shape

wine_data.isnull().sum()

wine_data.describe()

plt.subplots(figsize=(5,5))
sns.countplot(x=wine_data['Wine'])
print(wine_data['Wine'].value_counts())

'''Y = wine_data.loc[:,'Wine'].values
X = wine_data.loc[:,'Alcohol':'Proline'].values'''

X = wine_data[wine_data.columns[1:14]]
Y = wine_data[wine_data.columns[0]]

X.columns

# Split the dataset into a test and training set
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)

"""**LR Model with Hyper-Parameter Tuning**"""

model_LR = LogisticRegression(multi_class = 'ovr', max_iter=10000)

parameters = dict()
parameters['random_state'] = arange(0, 100, 10) # The seed of the pseudo random number generated which is used while shuffling the data
parameters['C'] = arange(0.01, 100, 10) # Inverse regularization parameter - A control variable that retains strength modification of Regularization by being inversely positioned to the Lambda regulator. C = 1/λ
parameters['solver'] = ['newton-cg', 'lbfgs', 'saga'] # Optimization
parameters['penalty'] = ['l1', 'l2'] # Penalization (Regularization).

## Building Grid Search algorithm with cross-validation and acc score.
grid_search_LR = GridSearchCV(estimator=model_LR, param_grid=parameters, scoring='accuracy', cv=5, n_jobs=-1)

## Lastly, finding the best parameters.
grid_search_LR.fit(X_train, y_train)
best_parameters_LR = grid_search_LR.best_params_  
best_score_LR = grid_search_LR.best_score_ 
print()
print(best_parameters_LR)
print(best_score_LR)

y_pred_1 = grid_search_LR.predict(X_test)

# Get the accuracy score
lr_acc = accuracy_score(y_test, y_pred_1)*100
lr_pre = precision_score(y_test, y_pred_1, average='micro')
lr_recall = recall_score(y_test, y_pred_1, average='micro')
lr_f1 = f1_score(y_test, y_pred_1, average='micro')

print("\nLR - Accuracy: {:.3f}.".format(lr_acc))
print("LR - Precision: {:.3f}.".format(lr_pre))
print("LR - Recall: {:.3f}.".format(lr_recall))
print("LR - F1 Score: {:.3f}.".format(lr_f1))
print ('\n Clasification Report:\n', classification_report(y_test,y_pred_1))

# Confusion Matrix
cm_1 = confusion_matrix(y_test, y_pred_1)
print(cm_1)

plt.figure(figsize=(5,5))
sns.heatmap(cm_1, annot=True, linewidths=.5, square = True, cmap = 'Blues_r');
plt.xlabel('Actual label');
plt.ylabel('Predicted label');
plt.title("Consfusion Matrix", size = 15);

def plot_multiclass_roc(grid_search_LR, X_test, y_test, n_classes, figsize=(17, 6)):
    y_score = grid_search_LR.decision_function(X_test)

    # structures
    fpr = dict()
    tpr = dict()
    roc_auc = dict()

    # calculate dummies once
    y_test_dummies = pd.get_dummies(y_test, drop_first=False).values
    for i in range(n_classes):
        fpr[i], tpr[i], _ = roc_curve(y_test_dummies[:, i], y_score[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])

    # roc for each class
    fig, ax = plt.subplots(figsize=figsize)
    ax.plot([0, 1], [0, 1], 'k--')
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.05])
    ax.set_xlabel('False Positive Rate')
    ax.set_ylabel('True Positive Rate')
    ax.set_title('Receiver operating characteristic example')
    for i in range(n_classes):
        ax.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f) for label %i' % (roc_auc[i], i))
    ax.legend(loc="best")
    ax.grid(alpha=.4)
    sns.despine()
    plt.show()

plot_multiclass_roc(grid_search_LR, X_test, y_test, n_classes=3, figsize=(8, 8))

"""**Class Balancing**"""

# pip install imbalanced-learn
from collections import Counter
from imblearn.over_sampling import SMOTE
X_b, y_b = SMOTE().fit_resample(X, Y)

plt.subplots(figsize=(5,5))
sns.countplot(x=y_b)
print(Counter(y_b))

# Split the dataset into a test and training set
X_tr, X_te, y_tr, y_te = train_test_split(X_b, y_b, test_size=0.2, random_state=0)

"""**LR Model with Class Balancing & Hyper-Parameter Tuning - Grid Search**"""

model_LR = LogisticRegression(multi_class = 'ovr', max_iter=10000)

parameters = dict()
parameters['random_state'] = arange(0, 100, 1) # The seed of the pseudo random number generated which is used while shuffling the data
parameters['C'] = arange(0.01, 100, 10) # Inverse regularization parameter - A control variable that retains strength modification of Regularization by being inversely positioned to the Lambda regulator. C = 1/λ
parameters['solver'] = ['newton-cg', 'lbfgs', 'saga'] # Optimization
parameters['penalty'] = ['l1', 'l2'] # Penalization (Regularization).


## Building Grid Search algorithm with cross-validation and acc score.
grid_search_LR_2 = GridSearchCV(estimator=model_LR, param_grid=parameters, scoring='accuracy', cv=5, n_jobs=-1)

## Lastly, finding the best parameters.
grid_search_LR_2.fit(X_tr, y_tr)
best_parameters_LR_2 = grid_search_LR_2.best_params_  
best_score_LR_2 = grid_search_LR_2.best_score_ 
print()
print(best_parameters_LR_2)
print(best_score_LR_2)

y_pred_2 = grid_search_LR_2.predict(X_te)

# Get the accuracy score
lr_acc_2 = accuracy_score(y_te, y_pred_2)*100
lr_pre_2 = precision_score(y_te, y_pred_2, average='micro')
lr_recall_2 = recall_score(y_te, y_pred_2, average='micro')
lr_f1_2 = f1_score(y_te, y_pred_2, average='micro')

print("\nLR - Accuracy: {:.3f}.".format(lr_acc_2))
print("LR - Precision: {:.3f}.".format(lr_pre_2))
print("LR - Recall: {:.3f}.".format(lr_recall_2))
print("LR - F1 Score: {:.3f}.".format(lr_f1_2))
print ('\n Clasification Report:\n', classification_report(y_te,y_pred_2))

# Confusion Matrix
cm_2 = confusion_matrix(y_te, y_pred_2)
print(cm_2)

import seaborn as sns
plt.figure(figsize=(5,5))
sns.heatmap(cm_2, annot=True, linewidths=.5, square = True, cmap = 'Blues_r');
plt.xlabel('Actual label');
plt.ylabel('Predicted label');
plt.title("Consfusion Matrix", size = 15);

def plot_multiclass_roc(grid_search_LR_2, X_te, y_te, n_classes, figsize=(8,8)):
    y_score_2 = grid_search_LR_2.decision_function(X_te)

    # structures
    fpr = dict()
    tpr = dict()
    roc_auc = dict()

    # calculate dummies once
    y_te_dummies = pd.get_dummies(y_te, drop_first=False).values
    for i in range(n_classes):
        fpr[i], tpr[i], _ = roc_curve(y_te_dummies[:, i], y_score_2[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])

    # roc for each class
    fig, ax = plt.subplots(figsize=figsize)
    ax.plot([0, 1], [0, 1], 'k--')
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.05])
    ax.set_xlabel('False Positive Rate')
    ax.set_ylabel('True Positive Rate')
    ax.set_title('Receiver operating characteristic example')
    for i in range(n_classes):
        ax.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f) for label %i' % (roc_auc[i], i))
    ax.legend(loc="best")
    ax.grid(alpha=.4)
    sns.despine()
    plt.show()

plot_multiclass_roc(grid_search_LR_2, X_te, y_te, n_classes=3, figsize=(8,8))

"""**Random Search**

In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions.
"""

from sklearn.model_selection import RandomizedSearchCV

model_LR = LogisticRegression(multi_class = 'ovr', max_iter=10000)

parameters = dict()
parameters['random_state'] = arange(0, 100, 1) # The seed of the pseudo random number generated which is used while shuffling the data
parameters['C'] = arange(0.01, 100, 10) # Inverse regularization parameter - A control variable that retains strength modification of Regularization by being inversely positioned to the Lambda regulator. C = 1/λ
parameters['solver'] = ['newton-cg', 'lbfgs', 'saga'] # Optimization
parameters['penalty'] = ['l1', 'l2'] # Penalization (Regularization).


## Building Grid Search algorithm with cross-validation and acc score.
rand_search_LR_2 = RandomizedSearchCV(model_LR, parameters, scoring='accuracy', cv=5, n_jobs=-1)

## Lastly, finding the best parameters.
rand_search_LR_2.fit(X_tr, y_tr)
best_parameters_LR_3 = rand_search_LR_2.best_params_  
best_score_LR_3 = rand_search_LR_2.best_score_ 
print()
print(best_parameters_LR_3)
print(best_score_LR_3)

y_pred_3 = rand_search_LR_2.predict(X_te)

# Get the accuracy score
lr_acc_3 = accuracy_score(y_te, y_pred_3)*100
lr_pre_3 = precision_score(y_te, y_pred_3, average='micro')
lr_recall_3 = recall_score(y_te, y_pred_3, average='micro')
lr_f1_3 = f1_score(y_te, y_pred_3, average='micro')

print("\nLR - Accuracy: {:.3f}.".format(lr_acc_3))
print("LR - Precision: {:.3f}.".format(lr_pre_3))
print("LR - Recall: {:.3f}.".format(lr_recall_3))
print("LR - F1 Score: {:.3f}.".format(lr_f1_3))
print ('\n Clasification Report:\n', classification_report(y_te,y_pred_3))

# Confusion Matrix
cm_3 = confusion_matrix(y_te, y_pred_3)
print(cm_3)

import seaborn as sns
plt.figure(figsize=(5,5))
sns.heatmap(cm_3, annot=True, linewidths=.5, square = True, cmap = 'Blues_r');
plt.xlabel('Actual label');
plt.ylabel('Predicted label');
plt.title("Consfusion Matrix", size = 15);